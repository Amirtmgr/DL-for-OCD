# ------------------------------------------------------------------------
# Description: Configuration file for the project
# Author: Amir Thapa Magar
# Email: amir.thapamagar(at)student.uni-siegen.de
# ------------------------------------------------------------------------

# Global variables

# Debugging options
debug: False

# Machine
machine: 'local' #'local' #'cluster'
world_size: 1

# Global variables
global:
  - &window_size 250
  - &num_epochs 25
  - &num_classes 2
  - &sensors 'both' #'acc' # 'gyro' # 'both'
  - &device 'cuda'
  - &num_workers 4
  - &random_seed 875
  - &use_warn_metrics True
  - &task_type -1 # -1 : rHW vs cHW #Binary | 0 : Null vs cHW | 1 : Null vs HW | 2 : rHW vs cHW | 3 : Null vs rHW vs cHW
  - &binary_threshold 0.5
  - &personalization False
  - &sampling True
  - &checkpoint '2023_10_10-03_39_29/cnn_transformer_cv-1_stratified_fold_2023-10-10_22-19-38.pth'
  
# Model architecture parameters
architecture:
  name: 'tinyhar' #'deepconvlstm' #'cnn_transformer' #'cnn'
  sensors: *sensors
  window_size: *window_size
  device: *device
  task_type: *task_type

  # For CNN
  input_channels: 1
  hidden_channels: [64, 64, 64, 64]
  kernel_sizes: [1, 1, 1, 1] 
  cnn_bias: False
  cnn_batch_norm: True

  # For LSTM
  lstm_hidden_size: 128
  lstm_num_layers: 2
  lstm_bias: False
  lstm_dropout: 0.0
  lstm_bidirectional: True

  # For Transformer
  multi_attn_heads: 32
  dim_feedforward: 128
  transformer_dropout: 0.1
  transformer_act_fn: 'gelu' #'gelu'
  num_encoder_layers: 6
  num_decoder_layers: 6
  encode_position: True
  
  # For TinyHAR
  tinyhar_filter_num: 64
  tinyhar_nb_conv_layers: 2
  tinyhar_filter_size: 1
  tinyhar_dropout: 0.25
  tinyhar_activation: "gelu" #'gelu'

  # For Dropout Layer
  dropout: 0.5
  
  # For FC layer or Head of Transformer
  fc_hidden_size: 256
  num_classes: *num_classes
  fc_batch_norm: False
  activation:
    name: 'gelu' #'leaky_relu' #'relu' #'tanh' #'sigmoid' #'elu'
    negative_slope: 0.01 # For leaky_relu
    alpha: 1.0 # For elu

  # For weights initialization
  scheme: 'xavier_uniform'

# Optimizer Hyperparameters
optim:
  name: 'adam' #'sgd'
  learning_rate: 0.0001
  weight_decay: 0.001
  momentum: 0.9
  nesterov: True

# Loss function parameters
criterion:
  name: 'bce' #'bce' #'cross_entropy'
  weighted: True
  reduction: 'mean'

# LR Scheduler parameters
lr_scheduler:
  name: 'reduce_lr_on_plateau' #'step_lr' #'reduce_lr_on_plateau'
  step_size: 10 # For step_lr
  gamma: 0.5 # For step_lr
  factor: 0.8 
  patience: 10 # For reduce_lr_on_plateau
  mode: 'min' # For reduce_lr_on_plateau
  verbose: True 
  threshold: 0.0001 # For reduce_lr_on_plateau

# Training Hyperparameters
train:
  device: *device
  num_epochs: *num_epochs
  batch_size: 32
  binary_threshold: *binary_threshold
  task_type: *task_type
  random_seed: *random_seed
  checkpoint: *checkpoint
  cross_validation:
    name: 'kfold' #'stratified' #'kfold' #'losocv'
    k_folds: 4
    
# Dataset parameters
dataset:
  name: 'OCDetect_raw_250' #'OCDetect_Export' #'OCDetect_raw_250' #'test_' #'datasets' #'OCDetect_Export' #'test' #'features' #"processed"
  sensor: *sensors #'acc' # 'gyro' # 'both'
  overlapping_ratio: 0.0
  window_size: *window_size
  num_classes: *num_classes
  labels: ['NUll', 'rHW', 'cHW']
  train_ratio: 0.7  # Highest 0.9
  split: 'subject_wise' # To:do future
  task_type: *task_type
  personalization: *personalization
  personalized_subject: 15
  random_seed: *random_seed
  shuffle: True
  batch_shuffle: False
  num_workers: *num_workers
  pin_memory: False
  scaler_type: #'Robust' #"MinMax" # "Standard" # "Robust"
  sampling: *sampling
  alpha: 0.015 # Sampling ratio

# Filter parameters
filter:
  fc_low: 0.3
  fc_high: 18.0
  order: 5
  sampling_rate: 50.0 # Hz

# Metrics parameters
metrics:
  zero_division: "warn"  #'nan' #'warn'#0 # 1
  use_warn_metrics: *use_warn_metrics
