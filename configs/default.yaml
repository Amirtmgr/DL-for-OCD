# ------------------------------------------------------------------------
# Description: Configuration file for the project
# Author: Amir Thapa Magar
# Email: amir.thapamagar(at)student.uni-siegen.de
# ------------------------------------------------------------------------

# Global variables

# Debugging options
debug: False

# Machine
machine: 'cluster' #'local' #'cluster'
world_size: 1

# Global variables
global:
  - &window_size 150
  - &num_epochs 100
  - &num_classes 2
  - &sensors 'both' #'acc' # 'gyro' # 'both'
  - &device 'cuda'
  - &num_workers 4
  - &random_seed 777
  - &use_warn_metrics True
  - &cHW_detection True
  - &binary_threshold 0.5
  - &personalization False
  - &sampling False
  - &checkpoint '2023_10_10-21_47_49/cnn_transformer_personalized_2023-10-10_21-53-07.pth' #'2023_10_10-03_54_17/cnn_transformer_cv-2_fold_2023-10-10_04-09-37.pth'
# Model architecture parameters
architecture:
  name: 'cnn_transformer' #'tinyhar' #'cnn_transformer' #'deepconvlstm' #'cnn_transformer' #'cnn'
  sensors: *sensors
  window_size: *window_size
  device: *device

  # For CNN
  input_channels: 1
  hidden_channels: [64, 64, 64, 64]
  kernel_sizes: [1, 1, 1, 1] 
  cnn_bias: False
  cnn_batch_norm: True

  # For LSTM
  lstm_hidden_size: 128
  lstm_num_layers: 2
  lstm_bias: False
  lstm_dropout: 0.0
  lstm_bidirectional: True

  # For Transformer
  multi_attn_heads: 8
  dim_feedforward: 256
  transformer_dropout: 0.5
  transformer_act_fn: 'gelu' #'gelu'
  num_encoder_layers: 6
  num_decoder_layers: 6
  encode_position: True
  
  # For TinyHAR
  tinyhar_filter_num: 64
  tinyhar_nb_conv_layers: 2
  tinyhar_filter_size: 3
  tinyhar_dropout: 0.1
  tinyhar_activation: "relu" #'gelu'
  
  # For Dropout Layer
  dropout: 0.5
  
  # For FC layer or Head of Transformer
  fc_hidden_size: 128
  num_classes: *num_classes
  fc_batch_norm: True
  activation:
    name: 'gelu' #'leaky_relu' #'relu' #'tanh' #'sigmoid' #'elu'
    negative_slope: 0.01 # For leaky_relu
    alpha: 1.0 # For elu

  # For weights initialization
  scheme: 'xavier_uniform'

# Optimizer Hyperparameters
optim:
  name: 'adam' #'sgd'
  learning_rate: 0.001
  weight_decay: 0.0001
  momentum: 0.9
  nesterov: True

# Loss function parameters
criterion:
  name: 'bce' #'bce' #'cross_entropy'
  weighted: True
  reduction: 'mean'

# LR Scheduler parameters
lr_scheduler:
  name: 'reduce_lr_on_plateau' #'step_lr' #'reduce_lr_on_plateau'
  step_size: 10 # For step_lr
  gamma: 0.1 # For step_lr
  factor: 0.95 
  patience: 7 # For reduce_lr_on_plateau
  mode: 'min' # For reduce_lr_on_plateau
  verbose: True 
  threshold: 0.0001 # For reduce_lr_on_plateau

# Training Hyperparameters
train:
  device: *device
  num_epochs: *num_epochs
  batch_size: 64
  binary_threshold: *binary_threshold
  cHW_detection: *cHW_detection
  random_seed: *random_seed
  cross_validation:
    name: 'loso' #'kfold' #'loso'
    k_folds: 4
  checkpoint: *checkpoint
  
# Dataset parameters
dataset:
  name: 'test_' #'processed' #'OCDetect_raw_250' #'test_' #'datasets' #'OCDetect_Export' #'test' #'features' #"processed"
  sensor: *sensors #'acc' # 'gyro' # 'both'
  overlapping_ratio: 0.0
  window_size: *window_size
  num_classes: *num_classes
  labels: ['NUll', 'rHW', 'cHW']
  train_ratio: 0.8  # Highest 0.9
  split: 'subject_wise' # To:do future
  personalization: *personalization
  personalized_subject: 15
  random_seed: *random_seed
  shuffle: False
  num_workers: *num_workers
  pin_memory: False
  scaler_type: #"MinMax" # "Standard" # "Robust"
  sampling: *sampling
  alpha: 0.75 # Sampling ratio

# Filter parameters
filter:
  fc_low: 0.3
  fc_high: 18.0
  order: 5
  sampling_rate: 50.0 # Hz

# Metrics parameters
metrics:
  zero_division: "warn"  #'nan' #'warn'#0 # 1
  use_warn_metrics: *use_warn_metrics
